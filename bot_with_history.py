import json
import discord
from discord.ext import commands
import requests
from langchain.memory import ConversationBufferMemory
import time

# æ¨¡å‹å°æ‡‰çš„æœ€å¤§ token é™åˆ¶
MODEL_MAX_TOKENS = {
    "gemma2:latest": 8192,
    "phi4:latest": 8192,
    "Qwen2.5:7b": 4096,
    "mistral:latest": 8192,
    "llama3.2:latest": 128000,
    "llama3.2-vision:latest": 128000,
    "deepseek-r1:1.5b": 128000,
    "deepseek-r1:latest": 128000,
    "deepseek-r1:8b": 128000,
    "deepseek-r1:14b": 128000
}

# åˆå§‹åŒ–è¨˜æ†¶åŠŸèƒ½
memory = ConversationBufferMemory(
    max_token_limit=8192)  # é»˜èªç‚º phi4 çš„æœ€å¤§ token é™åˆ¶

# è¼‰å…¥é…ç½®æ–‡ä»¶
with open("config.json", "r") as config_file:
    config = json.load(config_file)

# Discord Bot Token
DISCORD_TOKEN = config["DISCORD_TOKEN"]
# Ollama API URL
OLLAMA_API_URL = "http://localhost:11434/api/generate"

# æŒ‡å®šä¸Šç·šè¨Šæ¯çš„é »é“ ID
STATUS_CHANNEL_ID = 1330190647096905788  # æ›¿æ›ç‚ºä½ çš„é »é“ ID
ALLOWED_CHANNEL_ID = 1330190647096905788
# åˆå§‹åŒ– Bot
intents = discord.Intents.default()
intents.messages = True  # å•Ÿç”¨è¨Šæ¯äº‹ä»¶
intents.message_content = True  # å•Ÿç”¨è¨Šæ¯å…§å®¹è¨ªå•
bot = commands.Bot(command_prefix="++", intents=intents)

# å„²å­˜ç•¶å‰é¸æ“‡çš„æ¨¡å‹
current_model = "phi4:latest"  # é è¨­æ¨¡å‹


def is_in_allowed_channel(ctx):
    return ctx.channel.id == ALLOWED_CHANNEL_ID


def update_memory_limit():
    """æ ¹æ“šç•¶å‰æ¨¡å‹æ›´æ–°è¨˜æ†¶æœ€å¤§ token é™åˆ¶"""
    global memory
    max_tokens = MODEL_MAX_TOKENS.get(current_model, 8192)  # é»˜èªç‚º 8192
    memory = ConversationBufferMemory(max_token_limit=max_tokens)
    print(f"[DEBUG] è¨˜æ†¶æœ€å¤§ token é™åˆ¶æ›´æ–°ç‚º: {max_tokens}")


def save_history_to_file():
    """å°‡è¨˜æ†¶æ­·å²ä¿å­˜åˆ° JSON æ–‡ä»¶ä¸­"""
    context = memory.load_memory_variables({})
    with open("history.json", "w", encoding="utf-8") as history_file:
        json.dump(context, history_file, ensure_ascii=False, indent=4)
    print("[DEBUG] è¨˜æ†¶å·²ä¿å­˜åˆ° history.json")


def trim_memory_with_ollama():
    """ä½¿ç”¨ Ollama æ¨¡å‹è£å‰ªè¨˜æ†¶æ­·å²"""
    context = memory.load_memory_variables({})
    history = context.get("history", "")

    # å¦‚æœè¨˜æ†¶å¤ªçŸ­ï¼Œç„¡éœ€è£å‰ª
    if len(history.split("\n")) < 20:
        print("[DEBUG] è¨˜æ†¶å…§å®¹ä¸è¶³ä»¥è£å‰ªï¼Œè·³é")
        return

    # ç™¼é€è«‹æ±‚åˆ° Ollama æ¨¡å‹
    trim_prompt = f"ä»¥ä¸‹æ˜¯ç›®å‰çš„å°è©±æ­·å²ï¼Œè«‹é¸æ“‡å°è©±ä¸­æœ€é‡è¦çš„éƒ¨åˆ†ä¸¦ä¿ç•™ï¼š\n{history}\né‡è¦å°è©±ï¼š"
    response = requests.post(
        OLLAMA_API_URL,
        json={"model": current_model, "prompt": trim_prompt},
        headers={"Content-Type": "application/json"}
    )

    if response.status_code == 200:
        try:
            result = response.json()
            trimmed_history = result.get("response", "")
            print("[DEBUG] è£å‰ªå¾Œçš„è¨˜æ†¶æ­·å²ï¼š", trimmed_history)

            # æ›´æ–°è¨˜æ†¶
            memory.save_context({"input": ""}, {"output": trimmed_history})
            save_history_to_file()  # ä¿å­˜è£å‰ªå¾Œçš„è¨˜æ†¶
        except json.JSONDecodeError as e:
            print("[ERROR] ç„¡æ³•è§£æè£å‰ªå›æ‡‰ï¼š", e)
    else:
        print("[ERROR] Ollama API è¿”å›éŒ¯èª¤ï¼š", response.status_code, response.text)


def process_user_input(user_input):
    """è™•ç†ç”¨æˆ¶è¼¸å…¥ï¼Œä½¿ç”¨ Ollama API ä¸¦å„²å­˜è¨˜æ†¶"""
    try:
        # ç¢ºä¿è¨˜æ†¶åŠŸèƒ½åŒ…å«ä¸Šä¸‹æ–‡
        context = memory.load_memory_variables({})
        prompt_with_memory = context.get(
            "history", "") + f"\nUser: {user_input}\nBot:"

        print("[DEBUG] Prompt sent to Ollama API:", prompt_with_memory)
        start_time = time.time()
        full_prompt = f"å¦‚æˆ‘ç”¨ç¹é«”ä¸­æ–‡å•å•é¡Œï¼Œä¹Ÿè«‹ä½ ç”¨ç¹é«”ä¸­æ–‡å›ç­”ä»¥ä¸‹å•é¡Œä¸¦æŠŠå­—æ•¸æ§åˆ¶åœ¨30å­—ä»¥å…§ï¼Œä¸¦ä¸ä½¿ç”¨ä»»ä½•ç‰¹æ®Šå­—ç¬¦å’Œè¡¨æƒ…ï¼š{prompt_with_memory}"
        prompt_with_memory = full_prompt
        # ç™¼é€åˆ° Ollama API
        response = requests.post(
            OLLAMA_API_URL,
            json={"model": current_model, "prompt": prompt_with_memory},
            headers={"Content-Type": "application/json"}
        )

        if response.status_code == 200:
            try:
                # è¨ˆç®—è™•ç†æ™‚é–“
                elapsed_time = time.time() - start_time
                # æª¢æŸ¥æ˜¯å¦ç‚ºé€è¡Œ JSON å›æ‡‰
                if '\n' in response.text:
                    full_response = ""
                    for line in response.text.splitlines():
                        try:
                            data = json.loads(line)
                            full_response += data.get("response", "")
                            if data.get("done", False):
                                break
                        except json.JSONDecodeError:
                            continue
                    memory.save_context({"input": user_input}, {
                                        "output": full_response})
                    print("[DEBUG] Full response processed:", full_response)
                    save_history_to_file()  # ä¿å­˜è¨˜æ†¶æ­·å²
                    return full_response.strip(), elapsed_time
                else:
                    # å–®è¡Œ JSON å›æ‡‰
                    result = response.json()
                    bot_response = result.get("response", "æ¨¡å‹æœªè¿”å›å…§å®¹ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
                    # æ›´æ–°è¨˜æ†¶
                    memory.save_context({"input": user_input}, {
                                        "output": bot_response})
                    print("[DEBUG] Single-line response:", bot_response)
                    save_history_to_file()  # ä¿å­˜è¨˜æ†¶æ­·å²
                    return bot_response, elapsed_time
            except json.JSONDecodeError as e:
                raise Exception(f"JSON è§£ç¢¼éŒ¯èª¤ï¼š{e}")
        else:
            raise Exception(
                f"Ollama API Error: {response.status_code} - {response.text}"
            )
    except Exception as e:
        raise Exception(f"è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")


@bot.event
async def on_ready():
    """ç•¶ Bot ä¸Šç·šæ™‚è§¸ç™¼"""
    print("Bot å·²æˆåŠŸå•Ÿå‹•ï¼")
    print(f"å·²ç™»å…¥ Discord å¸³æˆ¶ï¼š{bot.user}")

    # ç™¼é€ä¸Šç·šé€šçŸ¥åˆ°æŒ‡å®šé »é“
    try:
        status_channel = bot.get_channel(STATUS_CHANNEL_ID)
        if status_channel:
            await status_channel.send("ğŸ¤– Bot å·²ä¸Šç·šï¼Œæº–å‚™æ¥æ”¶æŒ‡ä»¤ï¼")
        else:
            print(f"ç„¡æ³•æ‰¾åˆ°é »é“ IDï¼š{STATUS_CHANNEL_ID}")
    except Exception as e:
        print(f"ç™¼é€ä¸Šç·šé€šçŸ¥æ™‚å‡ºç¾éŒ¯èª¤ï¼š{e}")

bot.remove_command("help")


@bot.command()
@commands.check(is_in_allowed_channel)
async def help(ctx):
    """é¡¯ç¤ºå¯ç”¨æŒ‡ä»¤æ¸…å–®"""
    help_message = """
ğŸ¤– **å¯ç”¨æŒ‡ä»¤æ¸…å–®**:
1. **++chat <è¨Šæ¯>** - èˆ‡ Bot é€²è¡Œå°è©±ã€‚
2. **++setmodel <æ¨¡å‹åç¨±>** - é¸æ“‡è¦ä½¿ç”¨çš„æ¨¡å‹ã€‚
3. **++help** - é¡¯ç¤ºæ­¤å¹«åŠ©è¨Šæ¯ã€‚
4. **++clean_history** - æ¸…é™¤è¨˜æ†¶æ­·å²ã€‚

ğŸ“˜ **å¯ç”¨æ¨¡å‹**:
- é è¨­æ¨¡å‹: gemma2:latest
- Qwen2.5:7b      æ“…é•·ç·¨ç¢¼å’Œæ•¸å­¸èƒ½åŠ›
- gemma2:latest   æ“…é•·æ–‡æœ¬ç”Ÿæˆã€å°è©±ç³»çµ±
- mistral:latest  ä¸€èˆ¬ç”¨é€”
- phi4:latest     æ“…é•·æ–‡æœ¬ç”Ÿæˆã€å°è©±ç³»çµ±
- llama3.2:latest æ“…é•·å¤šèªè¨€æ”¯æŒã€å°è©±ç³»çµ±
- llama3.2-vision:latest  åœ–åƒè­˜åˆ¥ã€è¦–è¦ºæ¨ç†
- deepseek-r1:1.5b å¿«é€Ÿå›ç­” æœƒè¼¸å‡ºæ¨ç†(æ€è€ƒ)éç¨‹
- deepseek-r1:latest 7Bä¸­ç­‰è¤‡é›œåº¦ æœƒè¼¸å‡ºæ¨ç†(æ€è€ƒ)éç¨‹
- deepseek-r1:8b  æ•¸å­¸ç¨‹å¼é ˜åŸŸå‡ºè‰² æœƒè¼¸å‡ºæ¨ç†(æ€è€ƒ)éç¨‹
- deepseek-r1:14b é«˜ç­‰è¤‡é›œåº¦ æœƒè¼¸å‡ºæ¨ç†(æ€è€ƒ)éç¨‹
ğŸ¯ **ä½¿ç”¨æ–¹å¼**:
- è¼¸å…¥ `++chat ä½ å¥½` èˆ‡ Bot é–‹å§‹å°è©±ã€‚
- è¼¸å…¥ `++setmodel gemma2:latest` åˆ‡æ›åˆ°æŒ‡å®šçš„æ¨¡å‹ã€‚
- è¼¸å…¥ `++help` æŸ¥çœ‹å¯ç”¨æŒ‡ä»¤æ¸…å–®ã€‚
- è¼¸å…¥ `++clean_history` æ¸…é™¤è¨˜æ†¶æ­·å²ã€‚
"""
    await ctx.send(help_message)


@commands.check(is_in_allowed_channel)
@bot.command(name="chat")
async def chat(ctx, *, user_input: str):
    """è™•ç†èŠå¤©æŒ‡ä»¤"""
    try:
        print(f"æ”¶åˆ°æŒ‡ä»¤ï¼š{user_input}")
        thinking_message = await ctx.send(f"å·²æ”¶åˆ°ï¼š{user_input}ï¼Œæ­£åœ¨æ€è€ƒ...")

        # ç”Ÿæˆ Ollama å›æ‡‰
        response, _ = process_user_input(user_input)
        response = response.strip()
        await thinking_message.delete()

        if response and response != "æ¨¡å‹æœªè¿”å›å…§å®¹ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚":
            await ctx.send(response)
        else:
            await ctx.send("æ¨¡å‹æœªè¿”å›å…§å®¹æˆ–ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
    except Exception as e:
        print("[ERROR] Exception in chat command:", e)
        await ctx.send(f"å‡ºç¾éŒ¯èª¤ï¼š{e}")


@bot.command()
@commands.check(is_in_allowed_channel)
async def setmodel(ctx, model_name: str):
    """è¨­å®šä½¿ç”¨çš„æ¨¡å‹"""
    global current_model
    available_models = ["Qwen2.5:7b", "gemma2:latest",
                        "mistral:latest", "llama3.2:latest", "phi4:latest", "llama3.2-vision:latest", "deepseek-r1:latest", "deepseek-r1:1.5b", "deepseek-r1:14b", "deepseek-r1:8b"]
    if model_name in available_models:
        current_model = model_name
        update_memory_limit()  # æ›´æ–°è¨˜æ†¶é™åˆ¶
        print("[DEBUG] Model switched to:", model_name)
        await ctx.send(f"å·²å°‡æ¨¡å‹åˆ‡æ›ç‚º `{model_name}`ï¼Œè¨˜æ†¶æœ€å¤§é™åˆ¶æ›´æ–°ç‚º {MODEL_MAX_TOKENS[model_name]} tokensã€‚")
    else:
        print("[ERROR] Invalid model name:", model_name)
        await ctx.send(f"ç„¡æ•ˆçš„æ¨¡å‹åç¨±ï¼å¯ç”¨æ¨¡å‹ï¼š{', '.join(available_models)}")


@bot.command()
@commands.check(is_in_allowed_channel)
async def clean_history(ctx):
    """æ¸…é™¤è¨˜æ†¶æ­·å²"""
    global memory
    memory = ConversationBufferMemory(
        max_token_limit=MODEL_MAX_TOKENS.get(current_model, 8192))
    print("[DEBUG] è¨˜æ†¶æ­·å²å·²æ¸…é™¤")
    await ctx.send("è¨˜æ†¶æ­·å²å·²æˆåŠŸæ¸…é™¤ï¼")

bot.run(DISCORD_TOKEN)
