import json
import discord
from discord.ext import commands
import requests

# è¼‰å…¥é…ç½®æ–‡ä»¶
with open("config.json", "r") as config_file:
    config = json.load(config_file)

# Discord Bot Token
DISCORD_TOKEN = config["DISCORD_TOKEN"]
# Ollama API URL
OLLAMA_API_URL = "http://localhost:11434/api/generate"

# æŒ‡å®šä¸Šç·šè¨Šæ¯çš„é »é“ ID
STATUS_CHANNEL_ID = 1330190647096905788  # æ›¿æ›ç‚ºä½ çš„é »é“ ID

# åˆå§‹åŒ– Bot
intents = discord.Intents.default()
intents.messages = True  # å•Ÿç”¨è¨Šæ¯äº‹ä»¶
intents.message_content = True  # å•Ÿç”¨è¨Šæ¯å…§å®¹è¨ªå•
bot = commands.Bot(command_prefix="++", intents=intents)


@bot.event
async def on_ready():
    """ç•¶ Bot ä¸Šç·šæ™‚è§¸ç™¼"""
    print("Bot å·²æˆåŠŸå•Ÿå‹•ï¼")
    print(f"å·²ç™»å…¥ Discord å¸³æˆ¶ï¼š{bot.user}")

    # ç™¼é€ä¸Šç·šé€šçŸ¥åˆ°æŒ‡å®šé »é“
    try:
        status_channel = bot.get_channel(STATUS_CHANNEL_ID)
        if status_channel:
            await status_channel.send("ğŸ¤– Bot å·²ä¸Šç·šï¼Œæº–å‚™æ¥æ”¶æŒ‡ä»¤ï¼")
        else:
            print(f"ç„¡æ³•æ‰¾åˆ°é »é“ IDï¼š{STATUS_CHANNEL_ID}")
    except Exception as e:
        print(f"ç™¼é€ä¸Šç·šé€šçŸ¥æ™‚å‡ºç¾éŒ¯èª¤ï¼š{e}")

bot.remove_command("help")

ALLOWED_CHANNEL_ID = 1330190647096905788  # æ›¿æ›ç‚ºä½ çš„é »é“ ID


def is_in_allowed_channel(ctx):
    return ctx.channel.id == ALLOWED_CHANNEL_ID


@bot.command()
@commands.check(is_in_allowed_channel)
async def help(ctx):
    """é¡¯ç¤ºå¯ç”¨æŒ‡ä»¤æ¸…å–®"""
    help_message = """
ğŸ¤– **å¯ç”¨æŒ‡ä»¤æ¸…å–®**:
1. **++chat <è¨Šæ¯>** - èˆ‡ Bot é€²è¡Œå°è©±ã€‚
2. **++setmodel <æ¨¡å‹åç¨±>** - é¸æ“‡è¦ä½¿ç”¨çš„æ¨¡å‹ã€‚
3. **++help** - é¡¯ç¤ºæ­¤å¹«åŠ©è¨Šæ¯ã€‚

ğŸ“˜ **å¯ç”¨æ¨¡å‹**:
- é è¨­æ¨¡å‹: gemma2:latest
- æ¨¡å‹åç¨±	        max_token_limit  æ“…é•·é …ç›®
- Qwen2.5:7b       4096          ç·¨ç¢¼ç†è§£ã€æ•¸å­¸èƒ½åŠ›
- gemma2:latest    8192          æ–‡æœ¬ç”Ÿæˆã€é‚è¼¯æ¨ç†
- mistral:latest   8192          æ–‡æœ¬ç”Ÿæˆ
- llama3.2:latest  128000        æ•¸å­¸å•é¡Œã€å°è©±ç³»çµ±
- phi4:latest      8192          æ–‡æœ¬ç”Ÿæˆã€å°è©±ç³»çµ±

ğŸ¯ **ä½¿ç”¨æ–¹å¼**:
- è¼¸å…¥ `++chat ä½ å¥½` èˆ‡ Bot é–‹å§‹å°è©±ã€‚
- è¼¸å…¥ `++setmodel gemma2:latest` åˆ‡æ›åˆ°æŒ‡å®šçš„æ¨¡å‹ã€‚
- è¼¸å…¥ `++help` æŸ¥çœ‹å¯ç”¨æŒ‡ä»¤æ¸…å–®ã€‚
"""
    await ctx.send(help_message)

# å„²å­˜ç•¶å‰é¸æ“‡çš„æ¨¡å‹
current_model = "phi4:latest"  # é è¨­æ¨¡å‹


@bot.command()
@commands.check(is_in_allowed_channel)
async def setmodel(ctx, model_name: str):
    """è¨­å®šä½¿ç”¨çš„æ¨¡å‹"""
    global current_model
    # å¯ç”¨æ¨¡å‹æ¸…å–®
    available_models = ["Qwen2.5:7b", "gemma2:latest",
                        "mistral:latest", "llama3.2:latest", "phi4:latest"]

    if model_name in available_models:
        current_model = model_name
        await ctx.send(f"å·²å°‡æ¨¡å‹åˆ‡æ›ç‚º `{model_name}`")
        print(f"æ¨¡å‹åˆ‡æ›ç‚ºï¼š{model_name}")
    else:
        await ctx.send(f"ç„¡æ•ˆçš„æ¨¡å‹åç¨±ï¼å¯ç”¨æ¨¡å‹ï¼š{', '.join(available_models)}")
        print(f"ç„¡æ•ˆçš„æ¨¡å‹åç¨±ï¼š{model_name}")


@bot.command()
@commands.check(is_in_allowed_channel)
async def chat(ctx, *, user_input: str):
    """æ”¶åˆ°è¨Šæ¯å¾Œå…ˆå›è¦† 'å·²æ”¶åˆ°'ï¼Œä¸¦ä»¥ç¹é«”ä¸­æ–‡å›æ‡‰"""
    try:
        # åˆæ­¥å›æ‡‰ï¼šå·²æ”¶åˆ°è¨Šæ¯
        print(f"æ”¶åˆ°æŒ‡ä»¤ï¼š{user_input}")
        thinking_message = await ctx.send(f"å·²æ”¶åˆ°ï¼š{user_input}ï¼Œæ­£åœ¨ä½¿ç”¨ `{current_model}` æ¨¡å‹æ€è€ƒ...")

        # å¾Œå°æ‰“å°é€²åº¦
        print("å‘ Ollama API ç™¼é€è«‹æ±‚...")

        # å¢åŠ ç¹é«”ä¸­æ–‡çš„ä¸Šä¸‹æ–‡æŒ‡å¼•
        full_prompt = f"å¦‚æˆ‘ç”¨ç¹é«”ä¸­æ–‡å•å•é¡Œï¼Œä¹Ÿè«‹ä½ ç”¨ç¹é«”ä¸­æ–‡å›ç­”ä»¥ä¸‹å•é¡Œ ï¼Œä¸¦é¿å…ä½¿ç”¨ä»»ä½•ç‰¹æ®Šå­—ç¬¦ï¼š{user_input}"

        # å‘ Ollama API ç™¼é€è«‹æ±‚
        response = requests.post(
            OLLAMA_API_URL,
            json={"model": current_model, "prompt": full_prompt},
            headers={"Content-Type": "application/json"}
        )

        if response.status_code == 200:
            # è§£æé€è¡Œè¿”å›çš„çµæœ
            response.encoding = 'utf-8'  # æ˜ç¢ºæŒ‡å®šå›æ‡‰çš„ç·¨ç¢¼æ ¼å¼
            print("æ­£åœ¨è™•ç† API å›æ‡‰...")
            full_response = ""
            for line in response.text.splitlines():
                data = json.loads(line)
                full_response += data.get("response", "")  # ç²å–æ¯å€‹ç‰‡æ®µçš„å…§å®¹
                if data.get("done", False):  # å¦‚æœå®Œæˆï¼Œé€€å‡ºè§£æ
                    break

            # åˆªé™¤åˆæ­¥å›æ‡‰
            await thinking_message.delete()

            # ç™¼é€å®Œæ•´å›æ‡‰
            if full_response.strip():
                print(f"å®Œæˆï¼Œæ¨¡å‹å›æ‡‰å…§å®¹ï¼š{full_response}")
                # ç¢ºä¿å…§å®¹çš„ Unicode ç·¨ç¢¼æ­£å¸¸
                await ctx.send(full_response.encode('utf-8').decode('utf-8'))
            else:
                print("æ¨¡å‹æœªè¿”å›å…§å®¹")
                await ctx.send("æ¨¡å‹æœªè¿”å›å…§å®¹ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
        else:
            print(f"Ollama API è¿”å›éŒ¯èª¤ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
            await thinking_message.delete()
            await ctx.send(f"Ollama API è¿”å›éŒ¯èª¤ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
    except Exception as e:
        # ç™¼ç”ŸéŒ¯èª¤æ™‚ï¼Œåˆªé™¤åˆæ­¥å›æ‡‰
        print(f"è™•ç†è«‹æ±‚æ™‚å‡ºç¾éŒ¯èª¤ï¼š{e}")
        await thinking_message.delete()
        await ctx.send(f"å‡ºç¾éŒ¯èª¤ï¼š{e}")

bot.run(DISCORD_TOKEN)
